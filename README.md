# End-to-End Darts Analytics

Predictive Modeling des Future Dart Intelligence (FDI) Ratings mittels einer containerisierten Pipeline: Scraper (BeautifulSoup), Transformation, PostgreSQL, Scheduler, Training und Gradio-App laufen komplett in dieser Codebasis.

```
[ Scraper ] --> [ Transform ] --> [ Postgres DB ]
  |                     |                 |
  v                     v                 v
 data/raw/*.csv     data/processed/*.csv   Gradio API & UI
```

## Quick Start

1. **Dependencies installieren**
   ```bash
   uv sync
   cp .env.example .env
   ```
2. **Lokale Datenbank & App via Docker starten**
   ```bash
   docker compose build
   docker compose up
   ```
   - `db`: PostgreSQL 16 (persistentes Volume)
  - `etl`: fÃ¼hrt einmalig `pipeline.etl` aus (Scrape -> Transform -> Load)
   - `scheduler`: wiederholt denselben Job im Intervall `FDI_REFRESH_MINUTES`
   - `app`: Gradio + FastAPI Service
3. **Frontend Ã¶ffnen**: http://localhost:${APP_PORT} (Standard 7860)
  - Tab "Prediction Studio": manuelle Eingaben oder Spielerprofile auswÃ¤hlen und Ratings in Echtzeit berechnen.
  - Tab "Insights & EDA": Modell-Leaderboard, Top-Spieler und interaktive Scatterplots zur Kommunikation der Analyse-Erkenntnisse.

## Architektur im Detail

| Baustein | Beschreibung |
| --- | --- |
| `pipeline.scraper` | BeautifulSoup-Scraper fÃ¼r dartsorakel.com inkl. `--max-players` und optionalem Delay. |
| `pipeline.transform` | Bereinigt und feature-engineert die Scraper-Ausgabe (siehe `pipeline.features`). |
| `pipeline.ingest` | LÃ¤dt die veredelten CSV-Daten per SQLAlchemy in PostgreSQL. |
| `pipeline.etl` | Orchestriert Scrape -> Transform -> Load; wird vom Docker-Job und vom Scheduler verwendet. |
| `pipeline.scheduler` | FÃ¼hrt die ETL-Routine in einem konfigurierbaren Wochen/Minuten-Rhythmus aus und reagiert sauber auf SIGTERM. |
| `app/gradio_app.py` | Kombiniert Gradio-UI (Prediction Tab + Insights/EDA Tab), FastAPI-Endpunkte (`/api/*`) und das trainierte Pipeline-Modell. |

## VerzeichnisÃ¼berblick

```
â”œâ”€â”€ app/                 # Gradio + FastAPI Service
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Scraper-Output (`FDI_RAW_CSV_PATH`)
â”‚   â””â”€â”€ processed/       # Feature-engineerte Daten (`FDI_PROCESSED_CSV_PATH`)
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ scraper.py       # BeautifulSoup Scraper
â”‚   â”œâ”€â”€ transform.py     # Feature Engineering fÃ¼r Persistenz & Training
â”‚   â”œâ”€â”€ ingest.py        # Load in PostgreSQL
â”‚   â”œâ”€â”€ etl.py           # Orchestrierung + CLI
â”‚   â”œâ”€â”€ scheduler.py     # Cron-Ã¤hnlicher Loop
â”‚   â””â”€â”€ train.py         # Modelltraining (Linear Regression, Lasso, Random Forest)
â”œâ”€â”€ docker/              # App- & ETL-Dockerfiles
â”œâ”€â”€ models/              # Serialisierte Pipelines (`best_fdi_pipeline.joblib`)
â”œâ”€â”€ notebooks/           # EDA/Modellierung (Scraper-Notebook delegiert an pipeline.scraper)
â”œâ”€â”€ reports/             # Notebook-Reports & Metriken
â””â”€â”€ tests/               # Pytest-Suite (Feature Engineering & Ingestion)
```

## Datenpipeline bedienen

- **Einmaliger Lauf (z.â€†B. lokal)**
  ```bash
  uv run python -m pipeline.etl --max-players 250 --delay 0.25
  ```
  - `--skip-scrape`: vorhandene `data/raw/*.csv` wiederverwenden
  - `FDI_SCRAPE_MAX_PLAYERS`, `FDI_SCRAPE_DELAY_SECONDS`, `FDI_SKIP_SCRAPE` im `.env` setzbar
- **Nur Scraper anwerfen**
  ```bash
  uv run python -m pipeline.scraper --max-players 100 --output data/raw/custom.csv
  ```
- **Transformation isoliert testen**
  ```bash
  uv run python -m pipeline.transform --help  # oder direkt in Python importieren
  ```

## ðŸ“Š KPIs & Projekterfolg

Das Projekt wird anhand von 6 konkrete KPIs gemessen, die in [notebooks/fdi_rating_modeling.ipynb](notebooks/fdi_rating_modeling.ipynb) detailliert definiert sind:

| KPI | Ist-Zustand | Soll-Zustand | Status |
|-----|----------|----------|--------|
| **Modellgenauigkeit (MAE)** | 35.27 FDI-Punkte | < 40 | âœ… |
| **ErklÃ¤rte Varianz (RÂ²)** | 0.9286 | > 0.85 | âœ… |
| **CV-Robustheit (Ïƒ)** | Â±0.43 | < Â±5 | âœ… |
| **Feature-InterpretabilitÃ¤t** | Top 5 identified (First-9, Checkout, Legs-Win, log Earnings, Season-Win) | Explainable features | âœ… |
| **Residuen-Diagnostik** | Durbin-Watson=1.95, HeteroskedastizitÃ¤t r=-0.18, Cook's D 99.8% < Threshold | UnabhÃ¤ngig, homogen, no influential outliers | âœ… |
| **Production Readiness** | <100ms Inference, Gradio UI + Docker containerized, Weekly Scheduler | 24/7 VerfÃ¼gbarkeit | âœ… |

**Fazit:** Alle KPIs erfÃ¼llt oder Ã¼bertroffen â†’ **Projekt-Erfolg bestÃ¤tigt** âœ…

---

## Training & Modellvergleich

`pipeline/train.py` vergleicht drei AnsÃ¤tze (Linear Regression als Baseline, Lasso, Random Forest) Ã¼ber einen 80/20-Split und 5-fold Cross-Validation. FÃ¼r Lasso und Random Forest lÃ¤uft automatisch ein GridSearchCV-basiertes Hyperparameter-Tuning (ebenfalls 5-fold). Ergebnis:

```bash
uv run python -m pipeline.train \
  --csv data/processed/player_stats_all.csv \
  --model-path models/best_fdi_pipeline.joblib \
  --metrics-path reports/metrics/latest_metrics.json
```

- Das beste Modell wird als vollstÃ¤ndige `sklearn`-Pipeline persistiert und direkt von Gradio geladen.
- Die JSON-Metriken enthalten MAE, RMSE, $R^2$ sowie die CV-Ergebnisse aller Modelle und dienen als Audit-Log.

### Artefakte ohne Container-Rebuild aktualisieren

Der Gradio-Container mountet `models/` und `reports/metrics/` direkt:

```
app:
  volumes:
    - ./models:/app/models:ro
    - ./reports/metrics:/app/reports/metrics:ro
```

Workflow nach jedem Training:

1. `uv run python -m pipeline.train` â€” erzeugt `models/best_fdi_pipeline.joblib` und `reports/metrics/latest_metrics.json`.
2. Artefakte landen dank Mounts sofort im laufenden Container (kein `docker compose build` nÃ¶tig).
3. `docker compose restart app` (oder `docker compose up -d app`) lÃ¤dt das Modell neu; das Insights-Leaderboard liest automatisch die aktualisierte `latest_metrics.json`.

Optional kannst du die beiden Ordner auf S3/MinIO spiegeln, solange der Mount-Pfad auf dem Host weiterhin gefÃ¼llt wird.

## QualitÃ¤tssicherung

```bash
uv run pytest                # fÃ¼hrt die Tests im Ordner tests/ aus
uv run ruff check .          # statische Analyse
uv run ruff format .         # optionales Formatting
```

Die Tests prÃ¼fen u.â€¯a. das Feature-Engineering sowie das Chunk-Size-Handling beim Bulk-Insert.

## Lessons Learned (kurz)

- Log-Transformationen auf Earnings sind Pflicht, sonst dominieren AusreiÃŸer.
- Regularisierte lineare Modelle reichen aktuell aus; Random Forest brachte mit den vorhandenen Features keinen Zugewinn.
- Data Leakage vermeiden: API-Rankings (`api_rank`, `api_overall_stat`) frÃ¼hzeitig entfernen.
- Fehlwerte bei Alters-/Erfahrungsvariablen machen robuste Imputation (Median/Mode) nÃ¶tig und sollten explizit getrackt werden.

## Environment-Variablen

| Variable | Zweck |
| --- | --- |
| `FDI_RAW_CSV_PATH` | Speicherort des Scraper-Outputs (Standard `data/raw/player_stats_raw.csv`). |
| `FDI_PROCESSED_CSV_PATH` | Feature-engineerte CSV fÃ¼r Training & Ingestion. |
| `FDI_SCRAPE_MAX_PLAYERS` | Optionales Limit, z.â€¯B. fÃ¼r lokale Tests. Leer = alle Spieler. |
| `FDI_SCRAPE_DELAY_SECONDS` | Delay zwischen Requests zur Schonung des Targets. |
| `FDI_SKIP_SCRAPE` | `true`, um nur Transform + Load auszufÃ¼hren (nÃ¼tzlich bei reproduzierbaren Re-Runs). |
| `FDI_DB_CHUNKSIZE` | Chunk-GrÃ¶ÃŸe fÃ¼r `pandas.to_sql`. |
| `FDI_REFRESH_MINUTES` | Intervall des Docker-Schedulers. |
| `APP_HOST` / `APP_PORT` | Netzwerkeinstellungen der Gradio-App. |
| `MLFLOW_ENABLED` | Aktiviert das optionale MLflow-Tracking (`true`/`false`). |
| `MLFLOW_TRACKING_URI` | URI/Backend fÃ¼r MLflow, z.â€¯B. `http://localhost:5000`. |
| `MLFLOW_EXPERIMENT_NAME` | Experimentbezeichnung, falls Logging aktiv ist. |

Weitere Postgres-Parameter (`POSTGRES_*`, `DATABASE_URL`) werden direkt von Docker Compose Ã¼bernommen.

## Deployment mit Docker Compose

1. `.env` bereitstellen.
2. `docker compose build && docker compose up -d`
3. Logs prÃ¼fen: `docker compose logs -f scheduler`

Aktualisierte CSV-Dateien kannst du jederzeit mit `docker compose run --rm etl` sofort neu laden; der Scheduler Ã¼bernimmt ansonsten das wÃ¶chentliche Refresh automatisch.

---

**Tipp:** FÃ¼r lokale Experimente kannst du `uv run python app/gradio_app.py` starten. Die App verbindet sich automatisch mit der Postgres-Instanz (oder fÃ¤llt auf die CSV zurÃ¼ck) und stellt zusÃ¤tzlich die API-Endpunkte `/api/health`, `/api/players` und `/api/predict` bereit.
